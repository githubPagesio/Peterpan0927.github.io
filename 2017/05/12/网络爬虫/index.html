<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="how to reach the steins gate?">
    

    <!--Author-->
    
        <meta name="author" content="Peter pan">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="网络爬虫"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="how to reach the steins gate?" />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Peterpan&#39;s Blog"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>网络爬虫 - Peterpan&#39;s Blog</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2017/05/12/网络爬虫/">
                网络爬虫
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-05-12</span>
            
            
            
                <span class="category">
                    <a href="/categories/爬虫初步/">爬虫初步</a>
                </span>
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>学习笔记<br><a id="more"></a></p>
<h1 id="网络爬虫"><a href="#网络爬虫" class="headerlink" title="网络爬虫"></a>网络爬虫</h1><p>什么是爬虫？</p>
<p>网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动的抓取万维网信息的程序或者脚本。<br>简单的来说，爬虫就是把别人网站的信息弄到自己的电脑上，再做一些过滤，筛选，归纳，整理，排序等等，如果数据量足够大，算法足够好，能给别人提供优质的检索服务，就可以做成类似google或baidu</p>
<p>为什么选择python写爬虫？</p>
<p>1）抓取网页本身的接口<br>相比与其他静态编程语言，如java，c#，C++，python抓取网页文档的接口更简洁；相比其他动态脚本语言，如perl，shell，python的urllib2包提供了较为完整的访问网页文档的API。（当然ruby也是很好的选择）<br>此外，抓取网页有时候需要模拟浏览器的行为，很多网站对于生硬的爬虫抓取都是封杀的。这是我们需要模拟user agent的行为构造合适的请求，譬如模拟用户登陆、模拟session/cookie的存储和设置。在python里都有非常优秀的第三方包帮你搞定，如Requests，mechanize等等。</p>
<p>2）网页抓取后的处理<br>抓取的网页通常需要处理，比如过滤html标签，提取文本等。python的beautifulsoap提供了简洁的文档处理功能，能用极短的代码完成大部分文档的处理。</p>
<p>其实以上功能很多语言和工具都能做，但是用python能够干得最快，最干净。</p>
<h2 id="1-urlopen"><a href="#1-urlopen" class="headerlink" title="1.urlopen:"></a>1.urlopen:</h2><p>打开一个url方法，返回一个文件对象，然后就可以进行类似文件对象的操作。模块：urllib</p>
<p><img src="http://omg5mjb8v.bkt.clouddn.com/AE9908A0-44A5-4B78-8557-360650F1CFFF.png" alt=""></p>
<h2 id="2-urlretrieve"><a href="#2-urlretrieve" class="headerlink" title="2.urlretrieve():"></a>2.urlretrieve():</h2><p>urlretrieve方法将url定位到的html文件下载到你的本地硬盘当中,模块：utllib,当没有指定路径的时候可以放到临时路径下面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib</div><div class="line">a = urllib.urlretrieve(<span class="string">"xxx"</span>,filename = <span class="string">"/home/xx/xx/xx.xx"</span>)</div><div class="line"><span class="comment">#将a保存在本地硬盘中，可用的方法和urlopen相同,可以选择保存的路径</span></div></pre></td></tr></table></figure>
<h2 id="3-使用正则获取图片并保存在本地"><a href="#3-使用正则获取图片并保存在本地" class="headerlink" title="3.使用正则获取图片并保存在本地"></a>3.使用正则获取图片并保存在本地</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">imgList = re.findall(<span class="string">r'src="(.*?\.(jpg|png))"'</span>,html)</div><div class="line">x = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> imgurl <span class="keyword">in</span> imgList:</div><div class="line">    print(<span class="string">'正在下载%s'</span>%imgurl[<span class="number">0</span>])</div><div class="line">    urllib.urlretrieve(imgurl[<span class="number">0</span>],<span class="string">'./downloads/%d.jpg'</span>%x)</div><div class="line">    x += <span class="number">1</span></div></pre></td></tr></table></figure>
<h2 id="4-urlencode-GET和POST方法"><a href="#4-urlencode-GET和POST方法" class="headerlink" title="4.urlencode,GET和POST方法"></a>4.urlencode,GET和POST方法</h2><p>最重要的区别是GET方式是直接以链接形式访问，链接中包含了所有的参数，当然如果包含了密码的话是一种不安全的选择，不过你可以直观地看到自己提交了什么内容。POST则不会在网址上显示所有的参数，不过如果你想直接查看提交了什么就不太方便了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"> </div><div class="line">values = &#123;&#125;</div><div class="line">values[<span class="string">'username'</span>] = <span class="string">"1016903103@qq.com"</span></div><div class="line">values[<span class="string">'password'</span>] = <span class="string">"XXXX"</span></div><div class="line">data = urllib.urlencode(values) </div><div class="line">url = <span class="string">"http://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn"</span></div><div class="line">request = urllib2.Request(url,data)</div><div class="line">response = urllib2.urlopen(request)</div><div class="line"><span class="keyword">print</span> response.read()</div><div class="line"><span class="comment">#POST方法，构建request时传入两个参数，url和data</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"><span class="keyword">import</span> urllib</div><div class="line">values=&#123;&#125;</div><div class="line">values[<span class="string">'username'</span>] = <span class="string">"1016903103@qq.com"</span></div><div class="line">values[<span class="string">'password'</span>]=<span class="string">"XXXX"</span></div><div class="line">data = urllib.urlencode(values) </div><div class="line">url = <span class="string">"http://passport.csdn.net/account/login"</span></div><div class="line">geturl = url + <span class="string">"?"</span>+data</div><div class="line">request = urllib2.Request(geturl)</div><div class="line">response = urllib2.urlopen(request)</div><div class="line"><span class="keyword">print</span> response.read()</div><div class="line"><span class="comment">#GET方法，直接把参数写到网址上面，直接构建一个带参数的URL出来即可。</span></div></pre></td></tr></table></figure>
<h2 id="5-urllib2和伪造请求头部"><a href="#5-urllib2和伪造请求头部" class="headerlink" title="5.urllib2和伪造请求头部"></a>5.urllib2和伪造请求头部</h2><p>目的：是服务器分不清你是爬虫还是浏览器</p>
<p><img src="http://omg5mjb8v.bkt.clouddn.com/37300744-19C5-4DD5-8D90-606A58200F0A.png" alt=""></p>
<p>设置Headers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">url = <span class="string">'http://www.server.com/login'</span></div><div class="line">user_agent = <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4)'</span></div><div class="line">values = &#123;<span class="string">'username'</span> : <span class="string">'pzp'</span>, <span class="string">'password'</span> : <span class="string">'ascndksv'</span>&#125;</div><div class="line">headers = &#123;<span class="string">'User-Agent'</span> : user_agent&#125;</div><div class="line">data = urllib.urlencode(value)</div><div class="line">request = urllib.Request(url, data, headers)</div><div class="line">response = urllib.urlopen(request)</div><div class="line">page = response.read()</div><div class="line"><span class="comment">#服务器会识别headers中的referer是不是它自己，如果不是，有的服务器不会响应，所以我们还可以在headers中加入referer,这样就可以应付防盗链了</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span> : user_agent, <span class="string">'Referer'</span> : <span class="string">'xxxxxx'</span>&#125;</div></pre></td></tr></table></figure>
<p>关于headers的其他属性：</p>
<blockquote>
<p>User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求</p>
<p>Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。</p>
<p>application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用</p>
<p>application/json ： 在 JSON RPC 调用时使用</p>
<p>application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用</p>
<p>在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务</p>
</blockquote>
<h2 id="6-BeautifulSoup参考文章"><a href="#6-BeautifulSoup参考文章" class="headerlink" title="6.BeautifulSoup参考文章"></a>6.BeautifulSoup<a href="http://www.cnblogs.com/yupeng/p/3362031.html" target="_blank" rel="external">参考文章</a></h2><p>Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树(parse tree)。 它提供简单又常用的导航（navigating），搜索以及修改剖析树的操作。它可以大大节省你的编程时间。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">html = urllib2.urlopen(<span class="string">"xxxxx"</span>)</div><div class="line">html = html.read()</div><div class="line">soup = BeautifulSoup(html)</div><div class="line"><span class="comment">#下面是几个常用的功能</span></div><div class="line">soup.标签名  <span class="comment">#只会显示第一个</span></div><div class="line">soup.select(<span class="string">'标签名'</span>)</div><div class="line">soup.select(<span class="string">'.类名'</span>)</div><div class="line">soup.select(<span class="string">'#id名'</span>)</div><div class="line"><span class="comment">#可以通过标签名，类名，id名来寻找，可以找出所有的</span></div><div class="line">soup.select(<span class="string">'标签名 id'</span>)  <span class="comment">#组合查找</span></div><div class="line">soup.select(<span class="string">'head &gt; title'</span>) <span class="comment">#子标签查找</span></div><div class="line">soup.select(<span class="string">'a[class="sister"]'</span>)  <span class="comment">#属性查找</span></div><div class="line">soup.标签名.string   <span class="comment">#获取文字</span></div><div class="line">soup.标签名.attrs    <span class="comment">#获取属性</span></div><div class="line"><span class="comment">#通过遍历树来寻找</span></div><div class="line">soup.find_all(<span class="string">'name, attrs, recursive, text, limit, **kwargs'</span>)</div><div class="line"><span class="comment">#get_text</span></div><div class="line">soup.get_text()<span class="comment">#获取文字信息，类似于string，但是string只能对一个对象使用</span></div><div class="line">soup.stripped_string<span class="comment">#类似于get_text（）方法，但是会获取所有子标签的文字信息</span></div></pre></td></tr></table></figure></p>
<h2 id="7-使用select不断筛选-取得属性"><a href="#7-使用select不断筛选-取得属性" class="headerlink" title="7.使用select不断筛选,取得属性"></a>7.使用select不断筛选,取得属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#之前的过程省略,此时yy是一个列表，不能对他进行筛选操作</span></div><div class="line">yy = soup select(<span class="string">'div[id=xxx]'</span>)</div><div class="line"><span class="comment">#此时将列表又转换成了对象，可以继续操作了</span></div><div class="line">zz = yy[<span class="number">0</span>]</div><div class="line"><span class="comment">#指向性的提取对象中的属性</span></div><div class="line">zz[<span class="string">'href'</span>]</div><div class="line"><span class="comment">#将对象转换成列表</span></div><div class="line">list(xxx)</div></pre></td></tr></table></figure>
<h2 id="8-urlopen的分析"><a href="#8-urlopen的分析" class="headerlink" title="8.urlopen的分析"></a>8.urlopen的分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">urlopen(url, data, timeout)</div><div class="line"><span class="comment">#第一个参数url即为URL，第二个参数data是访问URL时要传送的数据，第三个timeout是设置超时时间。</span></div><div class="line"></div><div class="line"><span class="comment">#第二三个参数是可以不传送的，data默认为空None，timeout默认为 socket._GLOBAL_DEFAULT_TIMEOUT</span></div><div class="line"></div><div class="line"><span class="comment">#第一个参数URL是必须要传送的</span></div></pre></td></tr></table></figure>
<h2 id="9-构造Request"><a href="#9-构造Request" class="headerlink" title="9.构造Request"></a>9.构造Request</h2><p>其实上面的urlopen参数可以传入一个request请求,它其实就是一个Request类的实例，构造时需要传入Url,Data等等的内容。因为在构建请求时还需要加入好多内容，通过构建一个request，服务器响应请求得到应答，这样显得逻辑上清晰明确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"> </div><div class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com"</span>)</div><div class="line">response = urllib2.urlopen(request)</div><div class="line"><span class="keyword">print</span> response.read()</div></pre></td></tr></table></figure>
<h2 id="10-Proxy（代理）的设置"><a href="#10-Proxy（代理）的设置" class="headerlink" title="10.Proxy（代理）的设置"></a>10.Proxy（代理）的设置</h2><p>urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理，就不知道到底是谁了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">enable_proxy = <span class="keyword">True</span></div><div class="line">proxy_handler = urllib2.ProxyHandler(&#123;<span class="string">"http"</span> : <span class="string">'http://some-proxy.com:8080'</span>&#125;)</div><div class="line">null_proxy_handler = urllib2.ProxyHandler(&#123;&#125;)</div><div class="line"><span class="keyword">if</span> enable_proxy:</div><div class="line">    opener = urllib2.build_opener(proxy_handler)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    opener = urllib2.build_opener(null_proxy_handler)</div><div class="line">urllib2.install_opener(opener)</div></pre></td></tr></table></figure>
<h2 id="11-Timeout设置"><a href="#11-Timeout设置" class="headerlink" title="11.Timeout设置"></a>11.Timeout设置</h2><p>可以设置等待多久超时，为了解决一些网站实在响应过慢而造成的影响。如果第二个参数data为空那么要特别指定是timeout是多少，写明形参，如果data已经传入，则不必声明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">response = urllib2.urlopen(<span class="string">'http://www.baidu.com'</span>, data, <span class="number">10</span>)</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">response = urllib2.urlopen(<span class="string">'http://www.baidu.com'</span>, timeout=<span class="number">10</span>)</div></pre></td></tr></table></figure>
<h2 id="12-PUT和DELETE方法"><a href="#12-PUT和DELETE方法" class="headerlink" title="12.PUT和DELETE方法"></a>12.PUT和DELETE方法</h2><p>PUT：这个方法比较少见。HTML表单也不支持这个。本质上来讲， PUT和POST极为相似，都是向服务器发送数据，但它们之间有一个重要区别，PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定DELETE：删除某一个资源。基本上这个也很少见，不过还是有一些地方比如amazon的S3云服务里面就用的这个方法来删除资源。</p>
<h2 id="13-URLError"><a href="#13-URLError" class="headerlink" title="13.URLError"></a>13.URLError</h2><p>产生原因：</p>
<ul>
<li>网络无连接，即本机无法上网</li>
<li>连接不到特定的服务器</li>
<li>服务器不存在</li>
</ul>
<p>在代码中可以通过捕获异常来判断原因：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line">requset = urllib2.Request(<span class="string">'http://www.xxxxx.com'</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    urllib2.urlopen(request)</div><div class="line"><span class="keyword">except</span> urllib2.URLError, e:</div><div class="line">    <span class="keyword">print</span> e.reason</div><div class="line"><span class="comment">#如果访问了一个不存在的网址，那么运行的结果是：[Errno 11004] getaddrinfo failed</span></div></pre></td></tr></table></figure>
<p>HTTPError,是URLError的子类，所以也可以将父类捕获异常写在子类的后面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line">req = urllib2.Request(<span class="string">'http://blog.csdn.net/cqcre'</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    urllib2.urlopen(req)</div><div class="line"><span class="keyword">except</span> urllib2.HTTPError, e:</div><div class="line">    <span class="keyword">print</span> e.code</div><div class="line"><span class="keyword">except</span> urllib2.URLError, e:</div><div class="line">    <span class="keyword">print</span> e.reason</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">print</span> (<span class="string">"OK"</span>)</div><div class="line"><span class="comment">#样例检错：403</span></div></pre></td></tr></table></figure>
<h2 id="14-Opener概念"><a href="#14-Opener概念" class="headerlink" title="14.Opener概念"></a>14.Opener概念</h2><blockquote>
<p>Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib2库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。</p>
<p>当你获取一个URL你使用一个opener(一个urllib2.OpenerDirector的实例)。在前面，我们都是使用的默认的opener，也就是urlopen。它是一个特殊的opener，可以理解成opener的一个特殊实例，传入的参数仅仅是url，data，timeout。</p>
<p>如果我们需要用到Cookie，只用这个opener是不能达到目的的，所以我们需要创建更一般的opener来实现对Cookie的设置。</p>
</blockquote>
<h2 id="15-Cookielib"><a href="#15-Cookielib" class="headerlink" title="15.Cookielib"></a>15.Cookielib</h2><p>该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p>
<p>1.获取Cookie保存到变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"><span class="keyword">import</span> cookielib</div><div class="line"><span class="comment">#声明一个CookieJar对象实例来保存cookie</span></div><div class="line">cookie = cookielib.CookieJar()</div><div class="line"><span class="comment">#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器</span></div><div class="line">handler=urllib2.HTTPCookieProcessor(cookie)</div><div class="line"><span class="comment">#通过handler来构建opener</span></div><div class="line">opener = urllib2.build_opener(handler)</div><div class="line"><span class="comment">#此处的open方法同urllib2的urlopen方法，也可以传入request</span></div><div class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</div><div class="line">    <span class="keyword">print</span> <span class="string">'Name = '</span>+item.name</div><div class="line">    <span class="keyword">print</span> <span class="string">'Value = '</span>+item.value</div></pre></td></tr></table></figure>
<p>2.保存Cookie到文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cookielib</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line"><span class="comment">#设置保存cookie的文件，同级目录下的cookie.txt</span></div><div class="line">filename = <span class="string">'cookie.txt'</span></div><div class="line"><span class="comment">#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件</span></div><div class="line">cookie = cookielib.MozillaCookieJar(filename)</div><div class="line"><span class="comment">#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器</span></div><div class="line">handler = urllib2.HTTPCookieProcessor(cookie)</div><div class="line"><span class="comment">#通过handler来构建opener</span></div><div class="line">opener = urllib2.build_opener(handler)</div><div class="line"><span class="comment">#创建一个请求，原理同urllib2的urlopen</span></div><div class="line">response = opener.open(<span class="string">"http://www.baidu.com"</span>)</div><div class="line"><span class="comment">#保存cookie到文件</span></div><div class="line">cookie.save(ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>)</div><div class="line"><span class="comment">#ignore_discard的意思是即使cookies将被丢弃也将它保存下来</span></div><div class="line"><span class="comment">#ignore_expires的意思是如果该文件中的cookie已经存在，则覆盖原文件写入</span></div></pre></td></tr></table></figure>
<p>3.从文件中获取Cookie 值并访问</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cookielib</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line"><span class="comment">#创造一个MozillaCookieJar实例对象</span></div><div class="line">cookie = cookielib.MozillaCookieJar()</div><div class="line"><span class="comment">#从文件中读入值到实例对象</span></div><div class="line">cookie.load(<span class="string">'cookie.txt'</span>, ignore_disgard = <span class="keyword">True</span>, ignore_expires = <span class="keyword">True</span>)</div><div class="line"><span class="comment">#创造请求的request</span></div><div class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com"</span>)</div><div class="line"><span class="comment">#创建一个opener</span></div><div class="line">opener = urllib2.bulid_opener(urllib2.HTTPCookieProcessor(cookie))</div><div class="line">reponse = opener.open(request)</div><div class="line"><span class="keyword">print</span> reponse.read()</div></pre></td></tr></table></figure>
<p>4.利用cookie模拟网站的登录</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#示例。。登录校园网</span></div><div class="line"><span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> http.cookiejar</div><div class="line"></div><div class="line">filename = <span class="string">'cookie.txt'</span></div><div class="line"></div><div class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</div><div class="line"></div><div class="line">opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie))</div><div class="line"></div><div class="line">postdata = urllib.parse.urlencode(&#123;</div><div class="line">        <span class="string">'text'</span> : <span class="string">'2016xxxxxxxx'</span>,</div><div class="line">        <span class="string">'password'</span>   : <span class="string">'xxxxxxxxxxx'</span></div><div class="line">&#125;)</div><div class="line"></div><div class="line">loginurl = <span class="string">'http://ids.scuec.edu.cn/amserver/UI/Login?goto=http://eol.scuec.edu.cn/meol/homepage/common/sso_login.jsp'</span></div><div class="line"></div><div class="line">result = opener.open(loginurl, postdata.encode(<span class="string">'utf-8'</span>))</div><div class="line"></div><div class="line">cookie.save(ignore_discard=<span class="keyword">True</span>,ignore_expires=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">gradeurl = <span class="string">'http://eol.scuec.edu.cn/meol/jpk/course/layout/newpage/index.jsp?courseId=16574'</span></div><div class="line"></div><div class="line">result = opener.open(gradeurl)</div><div class="line"></div><div class="line"><span class="keyword">print</span> ((result.read()).decode(<span class="string">'gbk'</span>))</div></pre></td></tr></table></figure>
<h2 id="16-利用正则表达式"><a href="#16-利用正则表达式" class="headerlink" title="16.利用正则表达式"></a>16.利用正则表达式</h2><h4 id="1-定义：正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。"><a href="#1-定义：正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。" class="headerlink" title="1.定义：正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。"></a>1.定义：正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。</h4><p><img src="http://omg5mjb8v.bkt.clouddn.com/20130515113723855-e1424095177180.png" alt=""></p>
<h4 id="2-正则表达式的相关注解："><a href="#2-正则表达式的相关注解：" class="headerlink" title="2.正则表达式的相关注解："></a>2.正则表达式的相关注解：</h4><p>（1）数量词的贪婪模式与非贪婪模式</p>
<p>正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：正则表达式”ab<em>”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词”ab</em>?”，将找到”a”。</p>
<p>注：我们一般使用非贪婪模式来提取。</p>
<h3 id="（2）反斜杠问题"><a href="#（2）反斜杠问题" class="headerlink" title="（2）反斜杠问题"></a>（2）反斜杠问题</h3><p>与大多数编程语言相同，正则表达式里使用”\”作为转义字符，这就可能造成反斜杠困扰。假如你需要匹配文本中的字符”\”，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\”：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。</p>
<p>Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\”表示。同样，匹配一个数字的”\d”可以写成r”\d”。有了原生字符串，妈妈也不用担心是不是漏写了反斜杠，写出来的表达式也更直观了。</p>
<h2 id="17-两种表达方式"><a href="#17-两种表达方式" class="headerlink" title="17.两种表达方式"></a>17.两种表达方式</h2><p>XPath:/html/body/div[2]/ul/li[1]/img</p>
<p>CSS Selector:body &gt; div.main-content &gt; li:nth-child(1) &gt; img</p>
<h2 id="18-同步和异步加载"><a href="#18-同步和异步加载" class="headerlink" title="18.同步和异步加载"></a>18.同步和异步加载</h2><p>它允许无阻塞资源加载，并且使 onload 启动更快，允许页面内容加载，而不需要刷新页面，也可以根据页面内容延迟加载依赖。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//异步加载</span></div><div class="line">&lt;strong&gt;(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;  </div><div class="line">     <span class="keyword">var</span> s = <span class="built_in">document</span>.createElement(<span class="string">'script'</span>);  </div><div class="line">     s.type = <span class="string">'text/javascript'</span>;  </div><div class="line">     s.async = <span class="literal">true</span>;  </div><div class="line">     s.src = <span class="string">'http://yourdomain.com/script.js'</span>;  </div><div class="line">     <span class="keyword">var</span> x = <span class="built_in">document</span>.getElementsByTagName(<span class="string">'script'</span>)[<span class="number">0</span>];  </div><div class="line">     x.parentNode.insertBefore(s, x);  </div><div class="line"> &#125;)();<span class="xml"><span class="tag">&lt;/<span class="name">strong</span>&gt;</span></span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//同步加载</span></div><div class="line">&lt;script src=<span class="string">"http://XXX.com/script.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></div></pre></td></tr></table></figure>
<p>同步模式，又称阻塞模式，会阻止浏览器的后续处理，停止了后续的解析，因此停止了后续的文件加载（如图像）、渲染、代码执行。一般的script标签（不带async等属性）加载时会阻塞浏览器，也就是说，浏览器在下载或执行该js代码块时，后面的标签不会被解析，例如在head中添加一个script，但这个script下载时网络不稳定，很长时间没有下载完成对应的js文件，那么浏览器此时一直等待这个js文件下载，此时页面不会被渲染，用户看到的就是白屏。以前的一般建议是把<script>放在页面末尾</body>之前，这样尽可能减少这种阻塞行为，而先让页面展示出来。</p>
<h2 id="19-抓取异步加载的数据"><a href="#19-抓取异步加载的数据" class="headerlink" title="19.抓取异步加载的数据"></a>19.抓取异步加载的数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_more_data</span><span class="params">(start, end)</span>:</span></div><div class="line">    <span class="keyword">for</span> one <span class="keyword">in</span> range(start, end):</div><div class="line">        get_data(url+str(one))</div><div class="line">        tome.sleep(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<h2 id="20-使用MongoDB进行排版和插入"><a href="#20-使用MongoDB进行排版和插入" class="headerlink" title="20.使用MongoDB进行排版和插入"></a>20.使用MongoDB进行排版和插入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#使用mongodb进行简单的读取和插入</span></div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line"></div><div class="line">DB = client[<span class="string">'DB'</span>]</div><div class="line"></div><div class="line">sheet_line = DB[<span class="string">'sheet_line'</span>]</div><div class="line"></div><div class="line">path = <span class="string">'/Users/mac/Desktop/1.md'</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">    lines = f.readlines()</div><div class="line">    <span class="keyword">for</span> index,line <span class="keyword">in</span> enumerate(lines):</div><div class="line">        data=&#123;</div><div class="line">                <span class="string">'line'</span>  : line,</div><div class="line">                <span class="string">'index'</span> : index,</div><div class="line">                <span class="string">'words'</span> : len(line.split())</div><div class="line">             &#125;</div><div class="line">        print(data)</div><div class="line">        sheet_line.insert_one(data)</div></pre></td></tr></table></figure>
<p>几种表达式：</p>
<blockquote>
<p>$lt:less than</p>
<p>$gt:greater than</p>
<p>$lte:less than equal</p>
<p>$gte:greater than equal</p>
<p>$ne:not equal</p>
</blockquote>
<p>e.g-&gt;sheet.find{word:{‘$lt’:5}},表示找到sheet中所有字数比五小的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#找出字数小于等于三的行数并输出其内容</span></div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sheet_line.find&#123;word:&#123;<span class="string">'$lte'</span>:<span class="number">3</span>&#125;&#125;:</div><div class="line">    print(item[<span class="string">'line'</span>])</div></pre></td></tr></table></figure>
<h2 id="21-爬取大规模数据的工作流分析"><a href="#21-爬取大规模数据的工作流分析" class="headerlink" title="21.爬取大规模数据的工作流分析"></a>21.爬取大规模数据的工作流分析</h2><p><img src="http://omg5mjb8v.bkt.clouddn.com/7B2E42E6-4839-4CCB-81FB-9D4785BDDA12.png" alt=""></p>
<p>在爬取大规模数据的时候，要分模块的去爬取</p>
<p>1.构造一个爬取所有网页的爬虫，将爬取到的网页存储到数据库中</p>
<p>2.再构造一个爬虫从数据中提取网址，爬取单个页面的信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#可以通过find方法来对不同的网页来进行适配,e.g:</span></div><div class="line"><span class="keyword">if</span> soup.find(<span class="string">'td'</span>,<span class="string">'t'</span>):</div><div class="line">true<span class="comment">#进行相关的爬取操作</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">pass</span></div><div class="line"><span class="comment">#在对数据库进行插入操作的时候，也可以通过键值对的模式</span></div><div class="line"> sheet_line.insert_one(&#123;<span class="string">'url'</span> : <span class="string">'http://www.xxx.com'</span>&#125;)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将爬取的单个页面信息插入到数据库中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_info</span><span class="params">(url)</span>:</span></div><div class="line">    web_data = requests.get(url)</div><div class="line">    soup = BeautifulSoup(web_data.text, <span class="string">'lxml'</span>)</div><div class="line">    title = soup.title.text</div><div class="line">    price = soup.select(<span class="string">'span.price.c_f50'</span>)[<span class="number">0</span>].text</div><div class="line">    date = soup.select(<span class="string">'.time'</span>)[<span class="number">0</span>].text</div><div class="line">    <span class="comment">#进一步容错的设置</span></div><div class="line">    area = list(soup.select(<span class="string">'.c_25d a'</span>)[<span class="number">0</span>].stripped_strings) <span class="keyword">if</span> soup.find_all(<span class="string">'span'</span>,<span class="string">'c_25d'</span>) <span class="keyword">else</span> <span class="keyword">None</span></div><div class="line">    item_info.insert_one(&#123;<span class="string">'title'</span>:title, <span class="string">'date'</span>:date, <span class="string">'area'</span>:area&#125;)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#对于404页面的判断，e.g:</span></div><div class="line"><span class="comment">#404页面示例</span></div><div class="line">&lt;script src=<span class="string">"http://www.douyu.com/js/404/jQuery-1.3.2.js"</span> type= <span class="string">"text/javascript"</span>&gt;</div><div class="line">no_longer_exist = <span class="string">'404'</span> <span class="keyword">in</span> soup.find(<span class="string">'script'</span>, type = <span class="string">"text/javascript"</span>).get(<span class="string">'src'</span>).split(<span class="string">'/'</span>)</div><div class="line"><span class="comment">#返回一个布尔型来判断，加上一个判断语句加入爬取页面信息的函数即可判断404</span></div></pre></td></tr></table></figure>
<h2 id="22-进程和线程"><a href="#22-进程和线程" class="headerlink" title="22.进程和线程"></a>22.进程和线程</h2><h3 id="形象的理解方式："><a href="#形象的理解方式：" class="headerlink" title="形象的理解方式："></a>形象的理解方式：</h3><blockquote>
<p>单进程单线程：一个餐馆里一张桌子一个人吃饭</p>
<p>单进程多线程：一个餐馆里一张桌子多个人吃饭</p>
<p>多进程单线程：一个餐馆里多张桌子，每张桌子一个人吃饭</p>
<p>多进程多线程：一个餐馆里多张桌子，每个桌子多个人吃法</p>
</blockquote>
<h2 id="23-多进程爬虫数据抓取"><a href="#23-多进程爬虫数据抓取" class="headerlink" title="23.多进程爬虫数据抓取"></a>23.多进程爬虫数据抓取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#需要用到的库</span></div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="keyword">from</span> channel_extract <span class="keyword">import</span> channel_list</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_links_from</span><span class="params">(channel)</span>:</span></div><div class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">101</span>):</div><div class="line">        get_link_from(channel,num)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool()</div><div class="line">    pool.map(get_all_links_from,channel_list.split())</div></pre></td></tr></table></figure>
<h3 id="map函数"><a href="#map函数" class="headerlink" title="map函数"></a>map函数</h3><p>map(function,interable,…):对于可迭代函数’iterable’中的每一个元素应用’function’方法，将结果作为list返回</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">e.g1-&gt;def add100(x):</div><div class="line">    	return x + 100</div><div class="line">    hh = [11,22,33]</div><div class="line">    hhh = map(add100,hh)</div><div class="line">#此时hhh的值为[111,122,133]</div><div class="line">#如果给出了额外的可迭代参数，则对每个可迭代参数中的元素‘并行’的应用‘function’。</div><div class="line">e.g2-&gt;def abc(a,b,c)</div><div class="line">true  	return a*10000 + b*100 + c</div><div class="line">true  list1 = [11,22,33]</div><div class="line">      list2 = [44,55,66]</div><div class="line">      list3 = [77,88,99]</div><div class="line">      hh = map(abc,list1,list2,list3)</div><div class="line">#此时hh的值为[114477,225588,336699],在每个list中，取出了下标相同的元素，执行了abc()。</div><div class="line">#如果'function'给出的是‘None’，自动假定一个‘identity’函数</div><div class="line">&gt;&gt;&gt; list1 = [11,22,33]</div><div class="line">&gt;&gt;&gt; map(None,list1)</div><div class="line">[11, 22, 33]</div><div class="line">&gt;&gt;&gt; list1 = [11,22,33]</div><div class="line">&gt;&gt;&gt; list2 = [44,55,66]</div><div class="line">&gt;&gt;&gt; list3 = [77,88,99]</div><div class="line">&gt;&gt;&gt; map(None,list1,list2,list3)</div><div class="line">[(11, 44, 77), (22, 55, 88), (33, 66, 99)]</div></pre></td></tr></table></figure>
<h2 id="24-爬取大规模数据实例代码"><a href="#24-爬取大规模数据实例代码" class="headerlink" title="24.爬取大规模数据实例代码"></a>24.爬取大规模数据实例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#page_parsing.py</span></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line"></div><div class="line">ganji = client[<span class="string">'ganji'</span>]</div><div class="line"></div><div class="line">url_list = ganji[<span class="string">'url_list'</span>]</div><div class="line"></div><div class="line">item_info = ganji[<span class="string">'item_info'</span>]</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'</span>,</div><div class="line">    <span class="string">'Connection'</span> : <span class="string">'keep-alive'</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">proxy_list = [</div><div class="line">    <span class="string">'http://121.232.147.178:9000'</span>,</div><div class="line">    <span class="string">'http://122.243.11.57:9000'</span>,</div><div class="line">    <span class="string">'http://121.232.145.163:9000'</span></div><div class="line">]</div><div class="line">proxy_ip = random.choice(proxy_list)</div><div class="line">proxies = &#123;<span class="string">'http'</span> : proxy_ip&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_links_from</span><span class="params">(channel,pages,who_sells=<span class="string">'o'</span>)</span>:</span></div><div class="line">    list_view = <span class="string">'&#123;&#125;/&#123;&#125;&#123;&#125;/'</span>.format(channel, str(who_sells), str(pages))</div><div class="line">    wb_data = requests.get(channel, headers=headers)</div><div class="line">    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="keyword">if</span> soup.find(<span class="string">'ul'</span>, <span class="string">'pageLink'</span>):</div><div class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> soup.select(<span class="string">'td.t &gt; a.t'</span>):</div><div class="line">            item_link = link.get(<span class="string">'href'</span>).split(<span class="string">'?'</span>)[<span class="number">0</span>]</div><div class="line">            <span class="comment">#url_list.insert_one(&#123;'url':item_link&#125;)</span></div><div class="line">            <span class="comment">#print(item_link)</span></div><div class="line">            get_item_info_from(item_link)</div><div class="line">            print(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment">#已经到达最后一页</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_info_from</span><span class="params">(url, data=None)</span>:</span></div><div class="line">    wb_data = requests.get(url, headers=headers)</div><div class="line">    <span class="keyword">if</span> wb_data.status_code == <span class="number">404</span>:</div><div class="line">        <span class="keyword">pass</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</div><div class="line">            data = &#123;</div><div class="line">                <span class="string">'title'</span>:soup.title.text.strip(),</div><div class="line">                <span class="string">'price'</span>:soup.select(<span class="string">'.f22.fc-orange.f-type'</span>)[<span class="number">0</span>].text.strip(),</div><div class="line">                <span class="string">'pub_data'</span>:soup.select(<span class="string">'.pr-5'</span>)[<span class="number">0</span>].text.strip().split()[<span class="number">0</span>],</div><div class="line">                <span class="string">'area'</span>:list(map(<span class="keyword">lambda</span> x:x.text, soup.select(<span class="string">'ul.det-infor &gt; li &gt; a'</span>))),</div><div class="line">                <span class="string">'phoneNumber'</span>:soup.select(<span class="string">'span.phoneNum-style'</span>)[<span class="number">0</span>].text.strip(),</div><div class="line">                <span class="string">'url'</span>:url</div><div class="line">            &#125;</div><div class="line">            print(data)</div><div class="line">        <span class="comment">#except IndexError:</span></div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">except</span> AttributeError:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="comment">#get_item_info_from('http://bj.ganji.com/shouji/29096013665341x.htm')</span></div><div class="line"><span class="comment">#get_links_from('http://bj.ganji.com/shouji',2)</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#channel_extracing.py</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span>  BeautifulSoup</div><div class="line"></div><div class="line">start_url = <span class="string">'http://bj.ganji.com/wu'</span></div><div class="line">url_host = <span class="string">'http://bj.ganji.com'</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index_url</span><span class="params">(url)</span>:</span></div><div class="line">    wb_data = requests.get(url)</div><div class="line">    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</div><div class="line">    links = soup.select(<span class="string">'.fenlei &gt; dt &gt; a'</span>)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links:</div><div class="line">        page_url = url_host + link.get(<span class="string">'href'</span>)</div><div class="line">        print(page_url)</div><div class="line"></div><div class="line"><span class="comment">#get_index_url(start_url)</span></div><div class="line"></div><div class="line">channel_list = <span class="string">'''</div><div class="line">http://bj.ganji.com/jiaju/</div><div class="line">http://bj.ganji.com/rirongbaihuo/</div><div class="line">http://bj.ganji.com/shouji/</div><div class="line">http://bj.ganji.com/shoujihaoma/</div><div class="line">http://bj.ganji.com/bangong/</div><div class="line">http://bj.ganji.com/nongyongpin/</div><div class="line">http://bj.ganji.com/jiadian/</div><div class="line">http://bj.ganji.com/ershoubijibendiannao/</div><div class="line">http://bj.ganji.com/ruanjiantushu/</div><div class="line">http://bj.ganji.com/yingyouyunfu/</div><div class="line">http://bj.ganji.com/diannao/</div><div class="line">http://bj.ganji.com/xianzhilipin/</div><div class="line">http://bj.ganji.com/fushixiaobaxuemao/</div><div class="line">http://bj.ganji.com/meironghuazhuang/</div><div class="line">http://bj.ganji.com/shuma/</div><div class="line">http://bj.ganji.com/laonianyongpin/</div><div class="line">http://bj.ganji.com/xuniwupin/</div><div class="line">http://bj.ganji.com/qitawupin/</div><div class="line">http://bj.ganji.com/ershoufree/</div><div class="line">http://bj.ganji.com/wupinjiaohuan/</div><div class="line">'''</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#main.py</span></div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="keyword">from</span> channel_extracing <span class="keyword">import</span> channel_list</div><div class="line"><span class="keyword">from</span> page_parsing <span class="keyword">import</span> get_item_info_from,get_links_from,url_list,item_info</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_links</span><span class="params">(channel)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>):</div><div class="line">        get_links_from(channel, i)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool()</div><div class="line">    pool.map(get_all_links, channel_list.split())</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div></pre></td></tr></table></figure>
<h2 id="25-更新数据库"><a href="#25-更新数据库" class="headerlink" title="25.更新数据库"></a>25.更新数据库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">db.collection.update()</div><div class="line"><span class="comment">#update函数的用法,一般传入两个参数</span></div><div class="line">update(&#123;id:<span class="number">1</span>&#125;,&#123;$set:&#123;name:<span class="number">2</span>&#125;&#125;)</div></pre></td></tr></table></figure>
<h2 id="26-突破爬虫封禁的几种方法参考"><a href="#26-突破爬虫封禁的几种方法参考" class="headerlink" title="26.突破爬虫封禁的几种方法参考"></a>26.突破爬虫封禁的几种方法<a href="http://bigsec.com/bigsec-news/wechat-2016-web-crawler?ref=bigsec-news1">参考</a></h2><p>1.构造合理的HTTP头部请求</p>
<p>2.学会正确的设置cookie</p>
<p>3.正确的时间访问路径（不能访问过快）</p>
<p>4.隐含输入字段值（honey pot）</p>
<p>5.使用可变的远程ip（Tor代理服务器，防止ip被ban）</p>
<p>6.动态页面模拟人为操作（selenium+phantomJS框架）</p>
</script></p>
    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/Python/">#Python</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    Hacking to the gate
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/10/08/基于CVE-2017-7047的利用分析/">基于CVE-2017-7047d的利用分析</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/09/20/iPhone中新引入的保护机制/">iPhone中新引入的保护机制</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/09/10/empty-list分析/">empty_list分析</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/29/ZipperDown漏洞的新一轮分析/">ZipperDown漏洞的新一轮分析</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/UI学习-基础/">UI学习 基础</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/sumup/">sumup</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/动态规划/">动态规划</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/ARC自动回收机制/">ARC自动回收机制</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/peterpan0927">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/peterpan980927">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>