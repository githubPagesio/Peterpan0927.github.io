<html>
<head>
	
	<title>网络爬虫(二)</title>
	<meta name="keywords" content="fzb.me,冯宗宝,冯宗宝的blog" />

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
	   <link href="/css/main.css?v=3" rel="stylesheet" type="text/css" />
    
        <script src="/js/util.js"></script>
        <script>
            if(isMobile()) {
                loadjscssfile('../css/mobile.css', 'css');
            } else {
                loadjscssfile('../css/desktop.css', 'css');
            }
        </script> 
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/p1.png?v=3"/>
    

</head>

<body>


<h2 class="title">网络爬虫(二)</h2>
<!---
<div style="text-align:center;margin-top: -10px;">
<div class="article-category">
发表于2017年5月12日


    <a class="article-category-link" href="/categories/爬虫-MongoDB-charts/">爬虫 MongoDB charts</a>



 </div>
--->
</div>

<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-抓取异步加载的数据"><span class="toc-text">19.抓取异步加载的数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-使用MongoDB进行排版和插入"><span class="toc-text">20.使用MongoDB进行排版和插入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-爬取大规模数据的工作流分析"><span class="toc-text">21.爬取大规模数据的工作流分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-进程和线程"><span class="toc-text">22.进程和线程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#形象的理解方式："><span class="toc-text">形象的理解方式：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-多进程爬虫数据抓取"><span class="toc-text">23.多进程爬虫数据抓取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#map函数"><span class="toc-text">map函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-爬取大规模数据实例代码"><span class="toc-text">24.爬取大规模数据实例代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-更新数据库"><span class="toc-text">25.更新数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-突破爬虫封禁的几种方法参考"><span class="toc-text">26.突破爬虫封禁的几种方法参考</span></a></li></ol>
<p>学习笔记<br><a id="more"></a></p>
<h2 id="19-抓取异步加载的数据"><a href="#19-抓取异步加载的数据" class="headerlink" title="19.抓取异步加载的数据"></a>19.抓取异步加载的数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_more_data</span><span class="params">(start, end)</span>:</span></div><div class="line">    <span class="keyword">for</span> one <span class="keyword">in</span> range(start, end):</div><div class="line">        get_data(url+str(one))</div><div class="line">        tome.sleep(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<h2 id="20-使用MongoDB进行排版和插入"><a href="#20-使用MongoDB进行排版和插入" class="headerlink" title="20.使用MongoDB进行排版和插入"></a>20.使用MongoDB进行排版和插入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#使用mongodb进行简单的读取和插入</span></div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line"></div><div class="line">DB = client[<span class="string">'DB'</span>]</div><div class="line"></div><div class="line">sheet_line = DB[<span class="string">'sheet_line'</span>]</div><div class="line"></div><div class="line">path = <span class="string">'/Users/mac/Desktop/1.md'</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">    lines = f.readlines()</div><div class="line">    <span class="keyword">for</span> index,line <span class="keyword">in</span> enumerate(lines):</div><div class="line">        data=&#123;</div><div class="line">                <span class="string">'line'</span>  : line,</div><div class="line">                <span class="string">'index'</span> : index,</div><div class="line">                <span class="string">'words'</span> : len(line.split())</div><div class="line">             &#125;</div><div class="line">        print(data)</div><div class="line">        sheet_line.insert_one(data)</div></pre></td></tr></table></figure>
<p>几种表达式：</p>
<blockquote>
<p>$lt:less than</p>
<p>$gt:greater than</p>
<p>$lte:less than equal</p>
<p>$gte:greater than equal</p>
<p>$ne:not equal</p>
</blockquote>
<p>e.g-&gt;sheet.find{word:{‘$lt’:5}},表示找到sheet中所有字数比五小的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#找出字数小于等于三的行数并输出其内容</span></div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sheet_line.find&#123;word:&#123;<span class="string">'$lte'</span>:<span class="number">3</span>&#125;&#125;:</div><div class="line">    print(item[<span class="string">'line'</span>])</div></pre></td></tr></table></figure>
<h2 id="21-爬取大规模数据的工作流分析"><a href="#21-爬取大规模数据的工作流分析" class="headerlink" title="21.爬取大规模数据的工作流分析"></a>21.爬取大规模数据的工作流分析</h2><figure class="image-box">
                <img src="http://omg5mjb8v.bkt.clouddn.com/7B2E42E6-4839-4CCB-81FB-9D4785BDDA12.png" alt="" title="" class="">
                <p></p>
            </figure>
<p>在爬取大规模数据的时候，要分模块的去爬取</p>
<p>1.构造一个爬取所有网页的爬虫，将爬取到的网页存储到数据库中</p>
<p>2.再构造一个爬虫从数据中提取网址，爬取单个页面的信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#可以通过find方法来对不同的网页来进行适配,e.g:</span></div><div class="line"><span class="keyword">if</span> soup.find(<span class="string">'td'</span>,<span class="string">'t'</span>):</div><div class="line">	<span class="comment">#进行相关的爬取操作</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">pass</span></div><div class="line"><span class="comment">#在对数据库进行插入操作的时候，也可以通过键值对的模式</span></div><div class="line"> sheet_line.insert_one(&#123;<span class="string">'url'</span> : <span class="string">'http://www.xxx.com'</span>&#125;)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将爬取的单个页面信息插入到数据库中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_info</span><span class="params">(url)</span>:</span></div><div class="line">    web_data = requests.get(url)</div><div class="line">    soup = BeautifulSoup(web_data.text, <span class="string">'lxml'</span>)</div><div class="line">    title = soup.title.text</div><div class="line">    price = soup.select(<span class="string">'span.price.c_f50'</span>)[<span class="number">0</span>].text</div><div class="line">    date = soup.select(<span class="string">'.time'</span>)[<span class="number">0</span>].text</div><div class="line">    <span class="comment">#进一步容错的设置</span></div><div class="line">    area = list(soup.select(<span class="string">'.c_25d a'</span>)[<span class="number">0</span>].stripped_strings) <span class="keyword">if</span> soup.find_all(<span class="string">'span'</span>,<span class="string">'c_25d'</span>) <span class="keyword">else</span> <span class="keyword">None</span></div><div class="line">    item_info.insert_one(&#123;<span class="string">'title'</span>:title, <span class="string">'date'</span>:date, <span class="string">'area'</span>:area&#125;)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#对于404页面的判断，e.g:</span></div><div class="line"><span class="comment">#404页面示例</span></div><div class="line">&lt;script src=<span class="string">"http://www.douyu.com/js/404/jQuery-1.3.2.js"</span> type= <span class="string">"text/javascript"</span>&gt;</div><div class="line">no_longer_exist = <span class="string">'404'</span> <span class="keyword">in</span> soup.find(<span class="string">'script'</span>, type = <span class="string">"text/javascript"</span>).get(<span class="string">'src'</span>).split(<span class="string">'/'</span>)</div><div class="line"><span class="comment">#返回一个布尔型来判断，加上一个判断语句加入爬取页面信息的函数即可判断404</span></div></pre></td></tr></table></figure>
<h2 id="22-进程和线程"><a href="#22-进程和线程" class="headerlink" title="22.进程和线程"></a>22.进程和线程</h2><h3 id="形象的理解方式："><a href="#形象的理解方式：" class="headerlink" title="形象的理解方式："></a>形象的理解方式：</h3><blockquote>
<p>单进程单线程：一个餐馆里一张桌子一个人吃饭</p>
<p>单进程多线程：一个餐馆里一张桌子多个人吃饭</p>
<p>多进程单线程：一个餐馆里多张桌子，每张桌子一个人吃饭</p>
<p>多进程多线程：一个餐馆里多张桌子，每个桌子多个人吃法</p>
</blockquote>
<h2 id="23-多进程爬虫数据抓取"><a href="#23-多进程爬虫数据抓取" class="headerlink" title="23.多进程爬虫数据抓取"></a>23.多进程爬虫数据抓取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#需要用到的库</span></div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="keyword">from</span> channel_extract <span class="keyword">import</span> channel_list</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_links_from</span><span class="params">(channel)</span>:</span></div><div class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">101</span>):</div><div class="line">        get_link_from(channel,num)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool()</div><div class="line">    pool.map(get_all_links_from,channel_list.split())</div></pre></td></tr></table></figure>
<h3 id="map函数"><a href="#map函数" class="headerlink" title="map函数"></a>map函数</h3><p>map(function,interable,…):对于可迭代函数’iterable’中的每一个元素应用’function’方法，将结果作为list返回</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">e.g1-&gt;def add100(x):</div><div class="line">    	return x + 100</div><div class="line">    hh = [11,22,33]</div><div class="line">    hhh = map(add100,hh)</div><div class="line">#此时hhh的值为[111,122,133]</div><div class="line">#如果给出了额外的可迭代参数，则对每个可迭代参数中的元素‘并行’的应用‘function’。</div><div class="line">e.g2-&gt;def abc(a,b,c)</div><div class="line">	  	return a*10000 + b*100 + c</div><div class="line">	  list1 = [11,22,33]</div><div class="line">      list2 = [44,55,66]</div><div class="line">      list3 = [77,88,99]</div><div class="line">      hh = map(abc,list1,list2,list3)</div><div class="line">#此时hh的值为[114477,225588,336699],在每个list中，取出了下标相同的元素，执行了abc()。</div><div class="line">#如果'function'给出的是‘None’，自动假定一个‘identity’函数</div><div class="line">&gt;&gt;&gt; list1 = [11,22,33]</div><div class="line">&gt;&gt;&gt; map(None,list1)</div><div class="line">[11, 22, 33]</div><div class="line">&gt;&gt;&gt; list1 = [11,22,33]</div><div class="line">&gt;&gt;&gt; list2 = [44,55,66]</div><div class="line">&gt;&gt;&gt; list3 = [77,88,99]</div><div class="line">&gt;&gt;&gt; map(None,list1,list2,list3)</div><div class="line">[(11, 44, 77), (22, 55, 88), (33, 66, 99)]</div></pre></td></tr></table></figure>
<h2 id="24-爬取大规模数据实例代码"><a href="#24-爬取大规模数据实例代码" class="headerlink" title="24.爬取大规模数据实例代码"></a>24.爬取大规模数据实例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#page_parsing.py</span></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line"></div><div class="line">ganji = client[<span class="string">'ganji'</span>]</div><div class="line"></div><div class="line">url_list = ganji[<span class="string">'url_list'</span>]</div><div class="line"></div><div class="line">item_info = ganji[<span class="string">'item_info'</span>]</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'</span>,</div><div class="line">    <span class="string">'Connection'</span> : <span class="string">'keep-alive'</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">proxy_list = [</div><div class="line">    <span class="string">'http://121.232.147.178:9000'</span>,</div><div class="line">    <span class="string">'http://122.243.11.57:9000'</span>,</div><div class="line">    <span class="string">'http://121.232.145.163:9000'</span></div><div class="line">]</div><div class="line">proxy_ip = random.choice(proxy_list)</div><div class="line">proxies = &#123;<span class="string">'http'</span> : proxy_ip&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_links_from</span><span class="params">(channel,pages,who_sells=<span class="string">'o'</span>)</span>:</span></div><div class="line">    list_view = <span class="string">'&#123;&#125;/&#123;&#125;&#123;&#125;/'</span>.format(channel, str(who_sells), str(pages))</div><div class="line">    wb_data = requests.get(channel, headers=headers)</div><div class="line">    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="keyword">if</span> soup.find(<span class="string">'ul'</span>, <span class="string">'pageLink'</span>):</div><div class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> soup.select(<span class="string">'td.t &gt; a.t'</span>):</div><div class="line">            item_link = link.get(<span class="string">'href'</span>).split(<span class="string">'?'</span>)[<span class="number">0</span>]</div><div class="line">            <span class="comment">#url_list.insert_one(&#123;'url':item_link&#125;)</span></div><div class="line">            <span class="comment">#print(item_link)</span></div><div class="line">            get_item_info_from(item_link)</div><div class="line">            print(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment">#已经到达最后一页</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_info_from</span><span class="params">(url, data=None)</span>:</span></div><div class="line">    wb_data = requests.get(url, headers=headers)</div><div class="line">    <span class="keyword">if</span> wb_data.status_code == <span class="number">404</span>:</div><div class="line">        <span class="keyword">pass</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</div><div class="line">            data = &#123;</div><div class="line">                <span class="string">'title'</span>:soup.title.text.strip(),</div><div class="line">                <span class="string">'price'</span>:soup.select(<span class="string">'.f22.fc-orange.f-type'</span>)[<span class="number">0</span>].text.strip(),</div><div class="line">                <span class="string">'pub_data'</span>:soup.select(<span class="string">'.pr-5'</span>)[<span class="number">0</span>].text.strip().split()[<span class="number">0</span>],</div><div class="line">                <span class="string">'area'</span>:list(map(<span class="keyword">lambda</span> x:x.text, soup.select(<span class="string">'ul.det-infor &gt; li &gt; a'</span>))),</div><div class="line">                <span class="string">'phoneNumber'</span>:soup.select(<span class="string">'span.phoneNum-style'</span>)[<span class="number">0</span>].text.strip(),</div><div class="line">                <span class="string">'url'</span>:url</div><div class="line">            &#125;</div><div class="line">            print(data)</div><div class="line">        <span class="comment">#except IndexError:</span></div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">except</span> AttributeError:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="comment">#get_item_info_from('http://bj.ganji.com/shouji/29096013665341x.htm')</span></div><div class="line"><span class="comment">#get_links_from('http://bj.ganji.com/shouji',2)</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#channel_extracing.py</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span>  BeautifulSoup</div><div class="line"></div><div class="line">start_url = <span class="string">'http://bj.ganji.com/wu'</span></div><div class="line">url_host = <span class="string">'http://bj.ganji.com'</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index_url</span><span class="params">(url)</span>:</span></div><div class="line">    wb_data = requests.get(url)</div><div class="line">    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</div><div class="line">    links = soup.select(<span class="string">'.fenlei &gt; dt &gt; a'</span>)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links:</div><div class="line">        page_url = url_host + link.get(<span class="string">'href'</span>)</div><div class="line">        print(page_url)</div><div class="line"></div><div class="line"><span class="comment">#get_index_url(start_url)</span></div><div class="line"></div><div class="line">channel_list = <span class="string">'''</span></div><div class="line">http://bj.ganji.com/jiaju/</div><div class="line">http://bj.ganji.com/rirongbaihuo/</div><div class="line">http://bj.ganji.com/shouji/</div><div class="line">http://bj.ganji.com/shoujihaoma/</div><div class="line">http://bj.ganji.com/bangong/</div><div class="line">http://bj.ganji.com/nongyongpin/</div><div class="line">http://bj.ganji.com/jiadian/</div><div class="line">http://bj.ganji.com/ershoubijibendiannao/</div><div class="line">http://bj.ganji.com/ruanjiantushu/</div><div class="line">http://bj.ganji.com/yingyouyunfu/</div><div class="line">http://bj.ganji.com/diannao/</div><div class="line">http://bj.ganji.com/xianzhilipin/</div><div class="line">http://bj.ganji.com/fushixiaobaxuemao/</div><div class="line">http://bj.ganji.com/meironghuazhuang/</div><div class="line">http://bj.ganji.com/shuma/</div><div class="line">http://bj.ganji.com/laonianyongpin/</div><div class="line">http://bj.ganji.com/xuniwupin/</div><div class="line">http://bj.ganji.com/qitawupin/</div><div class="line">http://bj.ganji.com/ershoufree/</div><div class="line">http://bj.ganji.com/wupinjiaohuan/</div><div class="line">'''</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#main.py</span></div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="keyword">from</span> channel_extracing <span class="keyword">import</span> channel_list</div><div class="line"><span class="keyword">from</span> page_parsing <span class="keyword">import</span> get_item_info_from,get_links_from,url_list,item_info</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_links</span><span class="params">(channel)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>):</div><div class="line">        get_links_from(channel, i)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool()</div><div class="line">    pool.map(get_all_links, channel_list.split())</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div></pre></td></tr></table></figure>
<h2 id="25-更新数据库"><a href="#25-更新数据库" class="headerlink" title="25.更新数据库"></a>25.更新数据库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">db.collection.update()</div><div class="line"><span class="comment">#update函数的用法,一般传入两个参数</span></div><div class="line">update(&#123;id:<span class="number">1</span>&#125;,&#123;$set:&#123;name:<span class="number">2</span>&#125;&#125;)</div></pre></td></tr></table></figure>
<h2 id="26-突破爬虫封禁的几种方法参考"><a href="#26-突破爬虫封禁的几种方法参考" class="headerlink" title="26.突破爬虫封禁的几种方法参考"></a>26.突破爬虫封禁的几种方法<a href="http://bigsec.com/bigsec-news/wechat-2016-web-crawler?ref=bigsec-news1" target="_blank" rel="external">参考</a></h2><p>1.构造合理的HTTP头部请求</p>
<p>2.学会正确的设置cookie</p>
<p>3.正确的时间访问路径（不能访问过快）</p>
<p>4.隐含输入字段值（honey pot）</p>
<p>5.使用可变的远程ip（Tor代理服务器，防止ip被ban）</p>
<p>6.动态页面模拟人为操作（selenium+phantomJS框架）</p>


<!--<a href="http://yoursite.com/2017/05/12/网络爬虫-二/#disqus_thread" class="article-comment-link">Comments</a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'undefined'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
<div style="display:none">
<script src="http://s4.cnzz.com/stat.php?id=undefined&web_id=undefined" language="JavaScript"></script>script>
</div>






</body>
</html>