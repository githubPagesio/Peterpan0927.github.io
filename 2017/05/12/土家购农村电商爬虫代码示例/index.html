<html>
<head>
	
	<title>土家购农村电商爬虫代码示例</title>
	<meta name="keywords" content="fzb.me,冯宗宝,冯宗宝的blog" />

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
	   <link href="/css/main.css?v=3" rel="stylesheet" type="text/css" />
    
        <script src="/js/util.js"></script>
        <script>
            if(isMobile()) {
                loadjscssfile('../css/mobile.css', 'css');
            } else {
                loadjscssfile('../css/desktop.css', 'css');
            }
        </script> 
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/p1.png?v=3"/>
    

</head>

<body>


<h2 class="title">土家购农村电商爬虫代码示例</h2>
<!---
<div style="text-align:center;margin-top: -10px;">
<div class="article-category">
发表于2017年5月12日


    <a class="article-category-link" href="/categories/Python-爬虫-农村电商/">Python 爬虫 农村电商</a>



 </div>
--->
</div>


<p>网络爬虫<br><a id="more"></a></p>
<p>首先在pycharm这个IDE中写爬虫的主体部分:</p>
<pre><code class="python">#爬取链接和信息的部分
from bs4 import BeautifulSoup
import requests
import time
import pymongo
import random

#连接本地数据库并创建两个表
client = pymongo.MongoClient(&#39;localhost&#39;, 27017)
tujiagou = client[&#39;tujiagou&#39;]
url_list = tujiagou[&#39;url_list&#39;]
item_info = tujiagou[&#39;item_info&#39;]

start_url = &#39;http://www.tujiago.com/&#39;

headers = {
    &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36&#39;,
    &#39;Cookie&#39;:&#39;store_id=0; vary=staticdad85da5caa9112672513bb15a0d7952; UM_distinctid=15bf1838e3c153-064bb97bcefed4-153d655c-384000-15bf1838e3db2b; S[FIRST_REFER]=%7B%22ID%22%3A%22%22%2C%22REFER%22%3A%22https%3A%2F%2Fwww.google.co.jp%2F%22%2C%22DATE%22%3A1494404751000%7D; s=587fac54ade2a1a79d1a27819bdee8db; page_load_times=4; NTKF_T2D_CLIENTID=guestECE58596-E636-C47B-9C38-F1838DDD3CAE; nTalk_CACHE_DATA={uid:kf_9332_ISME9754_guestECE58596-E636-C4,tid:1494405582300579}; S[NOW_REFER]=%7B%22ID%22%3A%22%22%2C%22REFER%22%3A%22https%3A%2F%2Fwww.google.co.jp%2F%22%2C%22DATE%22%3A1494404751000%7D; S[N]=97FE7B11-7578-20E1-30BD-997B69666C35; CNZZDATA1259313336=1399165820-1494405492-null%7C1494405492&#39;,
    &#39;Referer&#39;:&#39;https://www.google.co.jp/&#39;
}

def get_links_from(channel):
    list_view = channel
    wb_data = requests.get(list_view,headers=headers)
    soup = BeautifulSoup(wb_data.text, &#39;lxml&#39;)
    for link in soup.select(&#39;.ui-page-num &gt; a&#39;):
            item_link = link.get(&#39;href&#39;)
            get_links_from_links(item_link)

def get_links_from_links(channel):
    list_view = channel
    wb_data = requests.get(list_view, headers=headers)
    soup = BeautifulSoup(wb_data.text, &#39;lxml&#39;)
    for item_link in soup.select(&#39;a.entry-title&#39;):
        item_link = start_url + item_link.get(&#39;href&#39;)
        time.sleep(1)
        get_item_info_from(item_link)

def get_item_info_from(url,data=None):
    wb_data = requests.get(url,headers=headers)
    if wb_data.status_code == 404:
        pass
    else:
        try:
            soup = BeautifulSoup(wb_data.text, &#39;lxml&#39;)
            data = {
                &#39;title&#39;:soup.title.text.strip(),
                &#39;price&#39;:soup.select(&#39;em.goodsprice&#39;)[0].text.strip(),
                &#39;area&#39;: soup.select(&#39;span.tb-deliveryAdd.deliveryAddModi&#39;)[0].text.strip(),
                &#39;view&#39;:list(soup.select(&#39;em.color_3355aa.evaluation&#39;)[0].stripped_strings),
                &#39;url&#39;:url
            }
        except IndexError:
            pass
        except AttributeError:
            pass
        print(data)
        if data:
            item_info.insert_one(data)
</code></pre>
<pre><code class="python">#主函数部分
from multiprocessing import Pool
from page_parsing import get_item_info_from,get_links_from_links,get_links_from
from channel_extracting import channel_list

if __name__ == &#39;__main__&#39;:
    pool = Pool(processes=6)
    pool.map(get_links_from,channel_list.split())
    pool.close()
    pool.join()
</code></pre>
<pre><code class="python">#爬取主页面链接
import requests
from bs4 import BeautifulSoup

start_url = &#39;http://www.tujiago.com/&#39;
def get_index_url(url):
    wb_data = requests.get(url)
    soup = BeautifulSoup(wb_data.text, &#39;lxml&#39;)
    links = soup.select(&#39;p.category-1 &gt; a&#39;)
    for link in links:
        page_url = start_url + link.get(&#39;href&#39;)
        print(page_url)

#get_index_url(start_url)

channel_list = &#39;&#39;&#39;
http://www.tujiago.com//index.php/gallery-656.html
http://www.tujiago.com//index.php/gallery-797.html
http://www.tujiago.com//index.php/gallery-699.html
http://www.tujiago.com//index.php/gallery-769.html
http://www.tujiago.com//index.php/gallery-689.html
http://www.tujiago.com//index.php/gallery-949.html
&#39;&#39;&#39;
</code></pre>
<pre><code class="python">#在jupyter notebook中的操作
import pymongo
import charts

client = pymongo.MongoClient(&#39;localhost&#39;, 27017)
tujiagou = client[&#39;tujiagou&#39;]
url_list = tujiagou[&#39;url_list&#39;]
item_info = tujiagou[&#39;item_info&#39;]

def data_gen():
    for i in item_info.find({},{&#39;price&#39;:1,&#39;view&#39;:1,&#39;title&#39;:1}).limit(6):
        data = {
            &#39;name&#39;: i[&#39;title&#39;],
            &#39;data&#39;: [int(i[&#39;view&#39;][0])],
            &#39;type&#39;: &#39;column&#39;
        }
        yield data

series = [i for i in data_gen()]
options = {
    &#39;chart&#39;   : {&#39;zoomType&#39;:&#39;xy&#39;},
    &#39;title&#39;   : {&#39;text&#39;: &#39;土家购浏览量与价钱的关系&#39;},
    &#39;subtitle&#39;: {&#39;text&#39;: &#39;柱状图分析&#39;},
    &#39;yAxis&#39;   : {&#39;title&#39;: {&#39;text&#39;: &#39;浏览量&#39;}},
    &#39;xAxis&#39;   : {&#39;categories&#39;: [&#39;商品类别&#39;]},
    }

charts.plot(series,options=options,show=&#39;inline&#39;)
</code></pre>
<p>最后会生成如下的图表效果：</p>
<p><img src="http://omg5mjb8v.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-12%20%E4%B8%8B%E5%8D%883.53.00.png" alt=""></p>


<!--<a href="http://yoursite.com/2017/05/12/土家购农村电商爬虫代码示例/#disqus_thread" class="article-comment-link">Comments</a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'undefined'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
<div style="display:none">
<script src="http://s4.cnzz.com/stat.php?id=undefined&web_id=undefined" language="JavaScript"></script>script>
</div>






</body>
</html>